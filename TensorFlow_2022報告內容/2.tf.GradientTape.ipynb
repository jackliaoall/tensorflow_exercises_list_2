{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.tf.GradientTape.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORxLdF+gGiYZV/CyBdxwxA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["參考: https://www.tensorflow.org/api_docs/python/tf/GradientTape?hl=zh_tw"],"metadata":{"id":"Ok2IKYBLPWSf"}},{"cell_type":"markdown","source":["Tensorflow一般使用tf.GradientTape來記錄前向傳遞(Forward propagation)過程，然後反向傳播(Backpropagation)自動得到梯度值。\n","\n","這種利用tf.GradientTape求微分的方法叫做Tensorflow的自動微分機制。\n","\n","公式：\n","f(x)=x^n\n","\n","微分(導數)：\n","f'(x)=n*x^(n-1)\n","\n","例：\n","y=x^2\n","微分(導數)：\n","dy/dx=2x^(2-1)=2x"],"metadata":{"id":"MLku-DU0KnaX"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","#https://blog.csdn.net/walilk/article/details/50978864"],"metadata":{"id":"ZBPAFqazKoOS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#For example, consider the function y = x * x. The gradient at x = 3.0 can be computed as:\n","x = tf.constant(3.0)\n","with tf.GradientTape() as g:\n","  g.watch(x)\n","  y = x * x\n","dy_dx = g.gradient(y, x)\n","\n","print(dy_dx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMHg6YJtPHrI","executionInfo":{"status":"ok","timestamp":1648558743507,"user_tz":-480,"elapsed":286,"user":{"displayName":"Liao Jack","userId":"16157886839679822522"}},"outputId":"03a4e305-ad95-4fa5-e48e-e41f809b8fa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(6.0, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","source":["#GradientTapes can be nested to compute higher-order derivatives. For example,\n","x = tf.constant(5.0)\n","with tf.GradientTape() as g:\n","  g.watch(x)\n","  with tf.GradientTape() as gg:\n","    gg.watch(x)\n","    y = x * x\n","  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n","d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n","print(dy_dx)\n","print(d2y_dx2)"],"metadata":{"id":"JBl5YaiyP8Zx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648555659427,"user_tz":-480,"elapsed":14,"user":{"displayName":"Liao Jack","userId":"16157886839679822522"}},"outputId":"12280f7e-a220-495c-b6e1-22f3d5531a5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(10.0, shape=(), dtype=float32)\n","tf.Tensor(2.0, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","source":["#By default GradientTape will automatically watch any trainable variables that are accessed inside the context. \n","#If you want fine grained control over which variables are watched you can disable automatic tracking \n","#by passing watch_accessed_variables=False to the tape constructor:\n","x = tf.Variable(2.0)\n","w = tf.Variable(5.0)\n","with tf.GradientTape(\n","    watch_accessed_variables=False, persistent=True) as tape:\n","  tape.watch(x)\n","  y = x ** 2  # Gradients will be available for `x`.\n","  z = w ** 3  # No gradients will be available as `w` isn't being watched.\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx)\n","\n","# No gradients will be available as `w` isn't being watched.\n","dz_dy = tape.gradient(z, w)\n","print(dz_dy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6Klu7RuQF85","executionInfo":{"status":"ok","timestamp":1648558944316,"user_tz":-480,"elapsed":2,"user":{"displayName":"Liao Jack","userId":"16157886839679822522"}},"outputId":"b1d7bad7-7627-4efb-8756-1ae00d065bbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(4.0, shape=(), dtype=float32)\n","None\n"]}]},{"cell_type":"markdown","source":["**Methods**\n","\n","batch_jacobian: Computes and stacks per-example jacobians."],"metadata":{"id":"i1J-kGTqUYOA"}},{"cell_type":"code","source":["with tf.GradientTape() as g:\n","  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n","  g.watch(x)\n","  y = x * x\n","batch_jacobian = g.batch_jacobian(y, x)\n","# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n","batch_jacobian"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUXZ6JyjS2XH","executionInfo":{"status":"ok","timestamp":1648555659427,"user_tz":-480,"elapsed":10,"user":{"displayName":"Liao Jack","userId":"16157886839679822522"}},"outputId":"dbeb0655-4ce1-4a14-8ff6-549f16627d21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n","array([[[2., 0.],\n","        [0., 4.]],\n","\n","       [[6., 0.],\n","        [0., 8.]]], dtype=float32)>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["[[1., 2.], [3., 4.]]\n","[1., 0.]\n","[0., 2.]"],"metadata":{"id":"jPAFYi0VzXrZ"}},{"cell_type":"markdown","source":["reset(): Clears all information stored in this tape."],"metadata":{"id":"NeEETcAWVNqO"}},{"cell_type":"code","source":["with tf.GradientTape() as t:\n","  loss = ...\n","  if loss > k:\n","    t.reset()"],"metadata":{"id":"IGt2mBxiVUMI","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"error","timestamp":1648555659716,"user_tz":-480,"elapsed":297,"user":{"displayName":"Liao Jack","userId":"16157886839679822522"}},"outputId":"8fa45a43-aaae-43c0-a3ac-e6d5fcf51c06"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a3548cc02c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"]}]},{"cell_type":"markdown","source":["**Method**\n","\n","stop_recording(): Temporarily stops recording operations on this tape.\n","\n","Operations executed while this context manager is active will not be recorded on the tape. This is useful for reducing the memory used by tracing all computations."],"metadata":{"id":"HcxdhbT7VcgB"}},{"cell_type":"code","source":["x = tf.constant(4.0)\n","\n","with tf.GradientTape() as tape:\n","  with tape.stop_recording():\n","    y = x ** 2\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx)"],"metadata":{"id":"L7Xc0ROOVsbk","executionInfo":{"status":"ok","timestamp":1648557145518,"user_tz":-480,"elapsed":268,"user":{"displayName":"Liao Jack","userId":"16157886839679822522"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2ba5168-84f4-40da-bbdb-e29a49743b1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]}]}